{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"In\u00edcio","text":"<ul> <li> <p>Buson App \u00e9 um aplicativo desenvolvido para facilitar o gerenciamento dos pagamentos mensais dos acad\u00eamicos que utilizam o servi\u00e7o de transporte para deslocamento at\u00e9 as universidades. Al\u00e9m disso, o aplicativo fornece informa\u00e7\u00f5es detalhadas sobre a associa\u00e7\u00e3o, situa\u00e7\u00e3o dos acad\u00eamicos e seus pagamentos por meio de um chat com intelig\u00eancia artificial.</p> </li> <li> <p>O projeto Buson Delta Lake se baseia em um banco de dados relacional criado na mat\u00e9ria de IA do Buson App, sendo utilizado como base para criar uma grande massa de dados e fazer uma ingest\u00e3o de dados usando o modelo Medalh\u00e3o. O objetivo final \u00e9 visualizar os dados em um BI.</p> </li> </ul>"},{"location":"configuracoesAirflow/","title":"Configura\u00e7\u00f5es Airflow e Minio","text":"<ul> <li>Acessar a seguinte url para realizar a execu\u00e7\u00e3o das DAGS do Airflow: http://localhost:8080/</li> <li>Usu\u00e1rio e senha de acesso ao airflow: admin e admin</li> <li>Configurar a conex\u00e3o com o cluster do spark na guia admin e posteriormente em conections (imagem da configura\u00e7\u00e3o enviada pelo Zanoni)</li> </ul>"},{"location":"configuracoesAirflow/#variaveis-de-ambiente-minio","title":"V\u00e1riaveis de Ambiente Minio","text":"<ul> <li>Acessar o minio na porta 9001 usuario: minioadmin senha: minioadmin</li> <li>Ir em AccessKeys e criar uma nova, copiar o accessKey e o accessSecretKey e colocar no dotenv do projeto. Apos isso reiniciar o ambiente com o comando  <pre><code>astro dev restart\n</code></pre></li> </ul>"},{"location":"configuracoesDocker/","title":"Configura\u00e7\u00f5es Docker","text":""},{"location":"configuracoesDocker/#dockerfile","title":"Dockerfile","text":"<ul> <li>Neste arquivo foi necess\u00e1rio realizar a instala\u00e7\u00e3o do manipulador do MinIO.</li> <li>Instala\u00e7\u00e3o do provider do Apache Spark para realizar a conex\u00e3o com o Airflow.</li> <li>Instala\u00e7\u00e3o da biblioteca pandas para manipula\u00e7\u00e3o dos dataframes.</li> <li>Instala\u00e7\u00e3o do Java e configura\u00e7\u00e3o das vari\u00e1veis de ambiente para o funcionamento do Spark.</li> </ul>"},{"location":"configuracoesDocker/#docker-compose-override","title":"Docker Compose Override","text":"<ul> <li>Realizado a inser\u00e7\u00e3o da imagem do MinIO e configura\u00e7\u00e3o da subnet para que o Airflow consiga enxerg\u00e1-lo.</li> <li>Realizado a inser\u00e7\u00e3o da imagem do cluster do Spark e configura\u00e7\u00e3o da subnet para que o Airflow consiga enxerg\u00e1-lo.</li> <li>Na imagem do PostgreSQL, adicionados os volumes do banco de dados <code>buson_bd</code> para realizar a cria\u00e7\u00e3o das tabelas e inser\u00e7\u00e3o autom\u00e1tica dos dados de exemplo.</li> </ul>"},{"location":"configuracoesDocker/#_1","title":"Configura\u00e7\u00f5es Docker","text":"<p>Obs: Essas configura\u00e7\u00f5es j\u00e1 est\u00e3o prontas na pasta do projeto</p>"},{"location":"dashboard/","title":"Dashboard","text":""},{"location":"dashboard/#configuracao-do-power-bi","title":"Configura\u00e7\u00e3o do Power BI","text":"<p>O arquivo j\u00e1 vem pr\u00e9 configurado para rodar localmente com o banco puxando do docker.</p> <p>A conex\u00e3o com o banco se faz por meio do Driver ODBC, caso n\u00e3o tenha instalado na maquina, precisa fazer o download.</p> <p>Ap\u00f3s a instala\u00e7\u00e3o do driver, quando abrir o arquivo no Power BI e tentar atualizar os dados, o mesmo ir\u00e1 solicitar a conex\u00e3o com o banco, onde o usuario padr\u00e3o \u00e9 postgres e a senha padr\u00e3o \u00e9 postgres. Caso possua um usuario postgres j\u00e1 cadastrado na maquina com informa\u00e7\u00f5es diferentes, utilizar o da maquina.</p>"},{"location":"dashboard/#execucao-do-dashboard","title":"Execu\u00e7\u00e3o do Dashboard","text":"<p>Ap\u00f3s a execu\u00e7\u00e3o completa da pipeline, pode-se abrir o arquivo no Power BI desktop e ir manualmente na consulta para atualizar o dados.</p> <p>O dashboard estar\u00e1 com os dados atualizados e pronto para realizar as an\u00e1lises.</p>"},{"location":"dashboard/#informacoes-do-dashboard","title":"Informa\u00e7\u00f5es do Dashboard","text":"<p>O dashboard foi feito pensado em facilitar a tomada de decis\u00e3o da presid\u00eancia de cada associa\u00e7\u00e3o, com 4 KPI's e 2 m\u00e9tricas.</p> <ul> <li> <p>KPI's: Quantidade de pagamentos em aberto, atrasados e pagos e o valor total arrecadado.</p> </li> <li> <p>M\u00e9tricas: Quantidade de pessoas por dias da semana e Valor arrecadado por m\u00eas.</p> </li> <li> <p>Filtros: Associa\u00e7\u00e3o e per\u00edodo (Ano).</p> </li> </ul>"},{"location":"dashboard/#exemplo-de-visualizacao","title":"Exemplo de Visualiza\u00e7\u00e3o:","text":""},{"location":"executando/","title":"Pipeline","text":"<ul> <li>Executar as seguintes DAGS na ordem aqui fornecida: </li> </ul>"},{"location":"executando/#bucketspy","title":"buckets.py","text":"<p>O MinIO Buckets realiza a cria\u00e7\u00e3o dos Buckets \"landing-zone\", \"bronze\", \"silver\" e \"gold\" no Object Storage. Dessa forma, os dados v\u00e3o ser persistidos ap\u00f3s manipulados.  <pre><code>def create_bucket():\n       client = Minio(MIN_HOST, access_key=MIN_ACCESS_KEY, secret_key=MIN_SECRET_KEY, secure=False)\n       minio_buckets = [\"landing-zone\", \"bronze\", \"silver\", \"gold\"]\n       for bucket in minio_buckets:\n           client.make_bucket(bucket)\n\n   create_bucket()\n</code></pre></p>"},{"location":"executando/#landing-zonepy","title":"landing-zone.py","text":"<p>Cria conex\u00e3o com o Postgres e extrai os dados em formato CSV.   <pre><code>file_names = ['./dags/csv/associacaos.csv', './dags/csv/instituicaos.csv', './dags/csv/cursos.csv', './dags/csv/usuarios.csv', './dags/csv/parametros.csv', './dags/csv/pagamentos.csv']\n\n       for file_name in file_names:\n           with open(file_name, 'w') as f:\n               pass\n\n       conn = psycopg2.connect(\n           dbname=\"postgres\",\n           user=\"postgres\",\n           password=\"postgres\",\n           host=\"host.docker.internal\",\n           port=\"5432\"\n       )\n\n       cur = conn.cursor()\n\n       # Fun\u00e7\u00e3o para exportar dados para CSV\n       def export_to_csv(table_name, file_name):\n           with open(file_name, 'w') as f:\n               cur.copy_expert(f'COPY {table_name} TO STDOUT WITH CSV HEADER', f)\n\n       # Exportando as tabelas para arquivos CSV\n       export_to_csv('associacaos', './dags/csv/associacaos.csv')\n       export_to_csv('instituicaos', './dags/csv/instituicaos.csv')\n       export_to_csv('cursos', './dags/csv/cursos.csv')\n       export_to_csv('usuarios', './dags/csv/usuarios.csv')\n       export_to_csv('parametros', './dags/csv/parametros.csv')\n       export_to_csv('pagamentos', './dags/csv/pagamentos.csv')\n\n       # Fechando o cursor e a conex\u00e3o\n       cur.close()\n       conn.close()\n</code></pre>  Por ser a Landing-Zone, esses dados ser\u00e3o persistidos em seu formato bruto, para serem manipulados nas camadas seguintes.  <pre><code># Bucket | Nome do Arquivo | Caminho do Arquivo\n       client.fput_object('landing-zone', 'associacaos.csv', './dags/csv/associacaos.csv')\n       client.fput_object('landing-zone', 'instituicaos.csv', './dags/csv/instituicaos.csv')\n       client.fput_object('landing-zone', 'cursos.csv', './dags/csv/cursos.csv')\n       client.fput_object('landing-zone', 'usuarios.csv', './dags/csv/usuarios.csv')\n       client.fput_object('landing-zone', 'parametros.csv', './dags/csv/parametros.csv')\n       client.fput_object('landing-zone', 'pagamentos.csv', './dags/csv/pagamentos.csv')\n</code></pre></p>"},{"location":"executando/#bronzepy","title":"bronze.py","text":"<p>A camada Bronze utiliza Spark para ler e manipular os dados da landing-zone.  <pre><code># Lendo o arquivo do bucket landing-zone\n   df_associacaos = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/associacaos.csv\") \n   df_cursos = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/cursos.csv\") \n   df_instituicaos = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/instituicaos.csv\") \n   df_pagamentos = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/pagamentos.csv\") \n   df_parametros = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/parametros.csv\") \n   df_usuarios = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/usuarios.csv\") \n</code></pre>  Para cada arquivo CSV, ser\u00e3o adicionadas novas colunas de metadados, marcando o momento do processamento,   e o nome do arquivo de origem.  <pre><code># Adicionando metadados de data e hora de processamento e nome do arquivo de origem\n   df_associacaos = df_associacaos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"associacaos.csv\"))\n   df_cursos = df_cursos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"cursos.csv\"))\n   df_instituicaos = df_instituicaos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"instituicaos.csv\"))\n   df_pagamentos = df_pagamentos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"pagamentos.csv\"))\n   df_parametros = df_parametros.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"parametros.csv\"))\n   df_usuarios = df_usuarios.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"usuarios.csv\"))\n</code></pre>  Ap\u00f3s esse processo, os dados s\u00e3o persistidos na camada Bronze, j\u00e1 em formato Delta.  <pre><code># Salvando arquivo em formato delta no bucket bronze\n   df_associacaos.write.format(\"delta\").save(f\"s3a://bronze/associacaos\")\n   df_cursos.write.format(\"delta\").save(f\"s3a://bronze/cursos\")\n   df_instituicaos.write.format(\"delta\").save(f\"s3a://bronze/instituicaos\")\n   df_pagamentos.write.format(\"delta\").save(f\"s3a://bronze/pagamentos\")\n   df_parametros.write.format(\"delta\").save(f\"s3a://bronze/parametros\")\n   df_usuarios.write.format(\"delta\").save(f\"s3a://bronze/usuarios\")\n</code></pre></p>"},{"location":"executando/#silverpy","title":"silver.py","text":"<p>A camada Silver utiliza Spark para ler e manipular os dados da camada Bronze.  <pre><code>spark = SparkSession.builder.config(conf=conf).getOrCreate()\n\n# Lendo os arquivos da camada bronze\n   df_associacaos = spark.read.format(\"delta\").load(f\"s3a://bronze/associacaos/\")\n   df_cursos = spark.read.format(\"delta\").load(f\"s3a://bronze/cursos/\")\n   df_instituicaos = spark.read.format(\"delta\").load(f\"s3a://bronze/instituicaos/\")\n   df_pagamentos = spark.read.format(\"delta\").load(f\"s3a://bronze/pagamentos/\")\n   df_parametros = spark.read.format(\"delta\").load(f\"s3a://bronze/parametros/\")\n   df_usuarios = spark.read.format(\"delta\").load(f\"s3a://bronze/usuarios/\")\n</code></pre>  Cria\u00e7\u00e3o dos metadados (colunas) referentes a data de processamento dos dados, e nome do arquivo de origem.  <pre><code># Adicionando metadados de data e hora de processamento e nome do arquivo de origem\n   df_associacaos = df_associacaos.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"associacaos\"))\n   df_cursos = df_cursos.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"cursos\"))\n   df_instituicaos = df_instituicaos.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"instituicaos\"))\n   df_pagamentos = df_pagamentos.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"pagamentos\"))\n   df_parametros = df_parametros.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"parametros\"))\n   df_usuarios = df_usuarios.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"usuarios\"))\n</code></pre>  Ap\u00f3s a cria\u00e7\u00e3o dos metadados, \u00e9 feita padroniza\u00e7\u00e3o dos nomes das tabelas, e exclus\u00e3o de tabelas desnecess\u00e1rias.  <pre><code>df_cursos = (\n   df_cursos\n   .withColumnRenamed(\"id\"                     , \"ID\")\n   .withColumnRenamed(\"nome\"                   , \"NOME\")\n   .withColumnRenamed(\"situacao\"               , \"SITUACAO\")\n   .withColumnRenamed(\"instituicao_id\"         , \"INSTITUICAO_ID\")\n   .drop(\"created_at\")\n   .drop(\"updated_at\")\n   .drop(\"data_hora_bronze\")\n   .withColumnRenamed(\"nome_arquivo\"           , \"NOME_ARQUIVO\")\n   .withColumnRenamed(\"data_hora_silver\"       , \"DATA_HORA_SILVER\")\n   )\n</code></pre>  As novas modifica\u00e7\u00f5es ser\u00e3o persistidas na camada Silver, j\u00e1 no formato Delta.  <pre><code># Salvando os arquivos na camada silver\n   df_associacaos.write.format(\"delta\").save(f\"s3a://silver/associacaos/\")\n   df_cursos.write.format(\"delta\").save(f\"s3a://silver/cursos/\")\n   df_instituicaos.write.format(\"delta\").save(f\"s3a://silver/instituicaos/\")\n   df_pagamentos.write.format(\"delta\").save(f\"s3a://silver/pagamentos/\")\n   df_parametros.write.format(\"delta\").save(f\"s3a://silver/parametros/\")\n   df_usuarios.write.format(\"delta\").save(f\"s3a://silver/usuarios/\")\n</code></pre></p>"},{"location":"executando/#goldpy","title":"gold.py","text":"<p>A camada Gold utiliza Spark para ler os dados da camada Silver.  <pre><code>spark = SparkSession.builder.config(conf=conf).getOrCreate()\n\n   # Lendo os arquivos da camada silver\n   df_associacaos = spark.read.format(\"delta\").load(f\"s3a://silver/associacaos/\")\n   df_cursos = spark.read.format(\"delta\").load(f\"s3a://silver/cursos/\")\n   df_instituicaos = spark.read.format(\"delta\").load(f\"s3a://silver/instituicaos/\")\n   df_pagamentos = spark.read.format(\"delta\").load(f\"s3a://silver/pagamentos/\")\n   df_parametros = spark.read.format(\"delta\").load(f\"s3a://silver/parametros/\")\n   df_usuarios = spark.read.format(\"delta\").load(f\"s3a://silver/usuarios/\") \n</code></pre>  Ser\u00e3o feitas modifica\u00e7\u00f5es em colunas espec\u00edficas dos dataframes, para que n\u00e3o haja ambiguidade no resultado.  <pre><code># Selecionando as colunas necess\u00e1rias e renomeando as colunas para evitar ambiguidades\n   df_associacaos_final = df_associacaos_clone.select(\"ID\", \"NOME\", \"SITUACAO\") \\\n       .withColumnRenamed(\"ID\", \"codigo_associacao\") \\\n       .withColumnRenamed(\"NOME\", \"nome_associacao\") \\\n       .withColumnRenamed(\"SITUACAO\", \"situacao_associacao\")\n\n   df_pagamentos_final = df_pagamentos_clone.select(\"ID\", \"DATA_VENCIMENTO\", \"VALOR\", \"MULTA\", \"SITUACAO\", \"USUARIO_ID\") \\\n       .withColumnRenamed(\"ID\", \"codigo_pagamento\") \\\n       .withColumnRenamed(\"DATA_VENCIMENTO\", \"data_vencimento\") \\\n       .withColumnRenamed(\"VALOR\", \"valor\") \\\n       .withColumnRenamed(\"MULTA\", \"multa\") \\\n       .withColumnRenamed(\"SITUACAO\", \"situacao\") \\\n       .withColumnRenamed(\"USUARIO_ID\", \"codigo_usuario\")\n</code></pre>  Ap\u00f3s manipular as colunas, os dataframes ser\u00e3o agregados em um \u00fanico dataframe.  <pre><code># Realizando a jun\u00e7\u00e3o dos dataframes\n   df_merged = df_pagamentos_final \\\n       .join(df_usuarios_final, df_pagamentos_final[\"codigo_usuario\"] == df_usuarios_final[\"codigo_usuario\"], \"inner\") \\\n       .join(df_associacaos_final, df_usuarios_final[\"codigo_associacao\"] == df_associacaos_final[\"codigo_associacao\"], \"inner\") \\\n       .select(df_pagamentos_final[\"codigo_pagamento\"],\n               df_pagamentos_final[\"data_vencimento\"],\n               df_pagamentos_final[\"valor\"],\n               df_pagamentos_final[\"multa\"],\n               df_pagamentos_final[\"situacao\"],\n               df_usuarios_final[\"codigo_usuario\"],\n               df_usuarios_final[\"nome_usuario\"],\n               df_usuarios_final[\"situacao_usuario\"],\n               df_usuarios_final[\"dias_uso_transporte\"],\n               df_associacaos_final[\"codigo_associacao\"],\n               df_associacaos_final[\"nome_associacao\"],\n               df_associacaos_final[\"situacao_associacao\"])\n</code></pre>  O resultado da jun\u00e7\u00e3o dos dataframes ser\u00e1 persistido na camada Gold  <pre><code>df_merged.write.format(\"delta\").save(f\"s3a://gold/modelo_eng_dados/\")\n</code></pre></p>"},{"location":"executando/#_1","title":"Pipeline","text":""},{"location":"executando/#_2","title":"Pipeline","text":"<p>Nessa parte descrita acima, acredito que pode ser colocado junto os comandos disponiveis nos arquivos, buckets.py, landzone.py, bronze.py,  silver.py e gold.py, os quais refltem todo o processo descrito acima como exemplifica\u00e7\u00e3o que est\u00e3o no caminho astro/dags</p>"},{"location":"geracaoDados/","title":"Gera\u00e7\u00e3o de Dados","text":""},{"location":"geracaoDados/#1-modelagem-do-banco-de-dados","title":"1. Modelagem do Banco de Dados","text":"<ul> <li>Foi criado um modelo inicial de banco de dados baseado no conceito do projeto Buson.</li> <li>O modelo incluiu defini\u00e7\u00f5es de tabelas, rela\u00e7\u00f5es e campos necess\u00e1rios para suportar as funcionalidades do aplicativo.</li> </ul>"},{"location":"geracaoDados/#2-desenvolvimento-do-backend-em-java","title":"2. Desenvolvimento do Backend em Java","text":"<ul> <li>Implementa\u00e7\u00e3o de um backend em Java para estabelecr a conex\u00e3o com o banco de dados PostgresSQL.</li> <li>Foram desenvolvidas entidades em Java que correspondem diretamente \u00e0s tabelas definidas no modelo do banco de dados.</li> </ul>"},{"location":"geracaoDados/#3-criacao-de-dados-ficticios","title":"3. Cria\u00e7\u00e3o de Dados Fict\u00edcios","text":"<ul> <li>Utiliza\u00e7\u00e3o da depend\u00eancia do Faker para gerar dados fict\u00edcios de maneira realista.</li> <li>Os dados fict\u00edcios foram utilizados para popular as tabelas do banco de dados durante o desenvolvimento e os testes iniciais.</li> </ul>"},{"location":"geracaoDados/#4-geracao-e-utilizacao-de-scripts-sql","title":"4. Gera\u00e7\u00e3o e Utiliza\u00e7\u00e3o de Scripts SQL","text":"<ul> <li>Foram gerados scripts SQL a partir das entidades Java desenvolvidas.</li> <li>Os scripts SQL incluem comandos para criar as tabelas necess\u00e1rias e inserir os dados fict\u00edcios gerados anteriormente.</li> <li>Esses scripts foram consolidados em um arquivo SQL \u00fanico para facilitar a inicializa\u00e7\u00e3o e a configura\u00e7\u00e3o do banco de dados em diferentes ambientes.</li> </ul>"},{"location":"geracaoDados/#5-configuracao-dos-ambientes-com-docker","title":"5. Configura\u00e7\u00e3o dos Ambientes com Docker","text":"<ul> <li>Na configura\u00e7\u00e3o dos ambientes de desenvolvimento, teste e produ\u00e7\u00e3o, foi utilizado Docker para garantir a consist\u00eancia e a portabilidade do ambiente.</li> <li>PostgreSQL foi escolhido como o banco de dados principal para integra\u00e7\u00e3o com os servi\u00e7os de Airflow e Minio.</li> <li>Um script de inicializa\u00e7\u00e3o foi preparado para automatizar a cria\u00e7\u00e3o das tabelas e a inser\u00e7\u00e3o dos dados no PostgreSQL, garantindo que o banco de dados esteja pronto para uso imediato nos ambientes Docker.</li> <li>Utilizou-se um volume inicial para disponibilizar o script de inicializa\u00e7\u00e3o, simplificando o processo de configura\u00e7\u00e3o e garantindo a replicabilidade do ambiente.</li> </ul>"},{"location":"inicializando/","title":"Inicializando","text":""},{"location":"inicializando/#como-subir-o-ambiente-com-airflow-spark-e-minio","title":"Como subir o ambiente com Airflow, Spark e Minio","text":""},{"location":"inicializando/#ferramentas-necessarias-para-a-criacao-do-ambiente","title":"Ferramentas necess\u00e1rias para a cria\u00e7\u00e3o do ambiente:","text":""},{"location":"inicializando/#docker","title":"Docker","text":"<p>Instale o Docker</p>"},{"location":"inicializando/#git","title":"Git","text":"<p>Instale o Git</p>"},{"location":"inicializando/#astro-cli","title":"Astro CLI","text":"<ul> <li>Abra o Windows PowerShell em modo Administrador e execute o seguinte comando:</li> </ul> <pre><code>   winget install -e --id Astronomer.Astro\n</code></pre> <p>Documenta\u00e7\u00e3o Astro CLI</p>"},{"location":"inicializando/#clonando-o-projeto","title":"Clonando o Projeto","text":"<ul> <li>Crie uma pasta no seu Desktop, e navegue at\u00e9 essa pasta atrav\u00e9s do CMD.</li> <li> <p>Ap\u00f3s navegar at\u00e9 a pasta, execute o seguinte comando no Terminal para clonar o reposit\u00f3rio:</p> <pre><code>git clone https://github.com/thiagoDimon/    buson-bd-eng-dados.git\n</code></pre> </li> </ul>"},{"location":"inicializando/#iniciando-as-imagens-do-container","title":"Iniciando as Imagens do Container","text":"<ul> <li>Abra o Docker Desktop, e certifique-se de estar logado em uma conta.</li> <li> <p>Pelo CMD, navegue at\u00e9 a pasta astro, e execute o seguinte comando no Terminal:</p> <pre><code>astro dev start\n</code></pre> </li> <li> <p>Agora, basta aguardar (pode demorar um pouco).</p> </li> </ul>"},{"location":"instalacao/","title":"Guia de Instala\u00e7\u00e3o","text":""},{"location":"preRequisitos/","title":"Pr\u00e9-Requisitos para o Projeto","text":"<p>Para garantir o funcionamento adequado do projeto, certifique-se de que todos os seguintes pr\u00e9-requisitos est\u00e3o atendidos e instalados:</p>"},{"location":"preRequisitos/#ferramentas-necessarias","title":"Ferramentas Necess\u00e1rias","text":"<ol> <li> <p>SPARK 3.3.1</p> <ul> <li>Utilizado para o processamento distribu\u00eddo de dados.</li> </ul> </li> <li> <p>DELTA 2.3.0</p> <ul> <li>Formato de armazenamento que permite a leitura e escrita de dados de forma eficiente com o Apache Spark.</li> </ul> </li> <li> <p>HADOOP 3.2.2</p> <ul> <li>Framework necess\u00e1rio para o funcionamento do Spark e Delta.</li> </ul> </li> <li> <p>MINIO OBJECT STORAGE</p> <ul> <li>Armazenamento de objetos onde os arquivos ser\u00e3o salvos.</li> </ul> </li> <li> <p>ASTRO CLI</p> <ul> <li>Interface de linha de comando para orquestra\u00e7\u00e3o de dados.</li> </ul> </li> <li> <p>AIRFLOW</p> <ul> <li>Ferramenta de orquestra\u00e7\u00e3o de workflows para agendar e monitorar os pipelines de dados.</li> </ul> </li> <li> <p>DOCKER</p> <ul> <li>Ferramenta para criar e gerenciar containers, utilizada para subir o Airflow, MinIO e o cluster do Spark.</li> </ul> </li> <li> <p>POWER BI</p> <ul> <li>Ferramenta utilizada para a cria\u00e7\u00e3o e visualiza\u00e7\u00e3o dos dados de forma visual.</li> </ul> </li> <li> <p>ODBC</p> <ul> <li>Driver PostgreSQL ANSI(x64) utilizado para a conex\u00e3o com o banco de dados Postgres.</li> </ul> </li> </ol>"},{"location":"preRequisitos/#ferramentas-adicionais-opcional","title":"Ferramentas Adicionais (Opcional)","text":"<p>As seguintes ferramentas foram utilizadas durante o desenvolvimento do projeto, mas n\u00e3o s\u00e3o necess\u00e1rias para a execu\u00e7\u00e3o:</p> <ul> <li> <p>GIT</p> <ul> <li>Sistema de controle de vers\u00e3o utilizado para clonar o reposit\u00f3rio do projeto.</li> </ul> </li> <li> <p>Java</p> <ul> <li>Utilizado para algumas depend\u00eancias do Spark.</li> </ul> </li> <li> <p>Faker</p> <ul> <li>Biblioteca para gera\u00e7\u00e3o de dados falsos para testes e desenvolvimento.</li> </ul> </li> </ul> <p>Certifique-se de que todas as ferramentas necess\u00e1rias est\u00e3o instaladas e configuradas corretamente antes de iniciar o projeto.</p>"}]}