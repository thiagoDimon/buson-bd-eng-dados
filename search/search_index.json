{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Projeto Engenharia de Dados","text":"<p>Trabalho desenvolvido para a disciplina de Engenharia de dados do Curso de Engenharia da Software da UNISATC. A proposta do projeto e desenvolver uma pipeline de engenharia de dados...</p>"},{"location":"#principais-comandos-do-mkdocs","title":"Principais comandos do MkDocs","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Cria um novo projeto.</li> <li><code>mkdocs serve</code> - Inicia o preview das paginas *.md da pastas /docs.</li> <li><code>mkdocs build</code> - Cria a estrutura de paginas web no padrao hmtl, css, js.</li> <li><code>mkdocs gh-deploy</code> - Publica as paginas criadas pelo 'mkdocs build' na estrutura do github pages.</li> </ul>"},{"location":"#documentacao-para-referencia-e-estudo","title":"Documentacao para referencia e estudo","text":"<p>https://tutorial-mkdocs.systemhealthlab.com/flavoured_markdown.html https://squidfunk.github.io/mkdocs-material/reference/</p>"},{"location":"configuracoesAirflow/","title":"Configura\u00e7\u00f5es Airflow e Minio","text":"<ul> <li>Acessar a seguinte url para realizar a execu\u00e7\u00e3o das DAGS do Airflow: http://localhost:8080/</li> <li>Usu\u00e1rio e senha de acesso ao airflow: admin e admin</li> <li>Configurar a conex\u00e3o com o cluster do spark na guia admin e posteriormente em conections (imagem da configura\u00e7\u00e3o enviada pelo Zanoni)</li> </ul>"},{"location":"configuracoesAirflow/#variaveis-de-ambiente-minio","title":"V\u00e1riaveis de Ambiente Minio","text":"<ul> <li>Acessar o minio na porta 9001 usuario: minioadmin senha: minioadmin</li> <li>Ir em AccessKeys e criar uma nova, copiar o accessKey e o accessSecretKey e colocar no dotenv do projeto. Apos isso reiniciar o ambiente com o comando  <pre><code>astro dev restart\n</code></pre></li> </ul>"},{"location":"configuracoesDocker/","title":"Configura\u00e7\u00f5es Docker","text":""},{"location":"configuracoesDocker/#dockerfile","title":"Dockerfile","text":"<ul> <li>Neste arquivo foi necess\u00e1rio realizar a instala\u00e7\u00e3o do manipulador do MinIO.</li> <li>Instala\u00e7\u00e3o do provider do Apache Spark para realizar a conex\u00e3o com o Airflow.</li> <li>Instala\u00e7\u00e3o da biblioteca pandas para manipula\u00e7\u00e3o dos dataframes.</li> <li>Instala\u00e7\u00e3o do Java e configura\u00e7\u00e3o das vari\u00e1veis de ambiente para o funcionamento do Spark.</li> </ul>"},{"location":"configuracoesDocker/#docker-compose-override","title":"Docker Compose Override","text":"<ul> <li>Realizado a inser\u00e7\u00e3o da imagem do MinIO e configura\u00e7\u00e3o da subnet para que o Airflow consiga enxerg\u00e1-lo.</li> <li>Realizado a inser\u00e7\u00e3o da imagem do cluster do Spark e configura\u00e7\u00e3o da subnet para que o Airflow consiga enxerg\u00e1-lo.</li> <li>Na imagem do PostgreSQL, adicionados os volumes do banco de dados <code>buson_bd</code> para realizar a cria\u00e7\u00e3o das tabelas e inser\u00e7\u00e3o autom\u00e1tica dos dados de exemplo.</li> </ul>"},{"location":"dashboard/","title":"Dashboard","text":""},{"location":"dashboard/#colocar-informacoes-sobre-o-power-bi","title":"COLOCAR INFORMACOES SOBRE O POWER BI","text":""},{"location":"executando/","title":"Executando o projeto","text":"<ul> <li>Executar as seguintes DAGS na ordem aqui fornecida: </li> </ul>"},{"location":"executando/#bucketspy","title":"buckets.py","text":"<p>O MinIO Buckets realiza a cria\u00e7\u00e3o dos Buckets \"landing-zone\", \"bronze\", \"silver\" e \"gold\" no Object Storage. Dessa forma, os dados v\u00e3o ser persistidos ap\u00f3s manipulados.  <pre><code>def create_bucket():\n       client = Minio(MIN_HOST, access_key=MIN_ACCESS_KEY, secret_key=MIN_SECRET_KEY, secure=False)\n       minio_buckets = [\"landing-zone\", \"bronze\", \"silver\", \"gold\"]\n       for bucket in minio_buckets:\n           client.make_bucket(bucket)\n\n   create_bucket()\n</code></pre></p>"},{"location":"executando/#landing-zonepy","title":"landing-zone.py","text":"<p>Cria conex\u00e3o com o Postgres e extrai os dados em formato CSV.   <pre><code>file_names = ['./dags/csv/associacaos.csv', './dags/csv/instituicaos.csv', './dags/csv/cursos.csv', './dags/csv/usuarios.csv', './dags/csv/parametros.csv', './dags/csv/pagamentos.csv']\n\n       for file_name in file_names:\n           with open(file_name, 'w') as f:\n               pass\n\n       conn = psycopg2.connect(\n           dbname=\"postgres\",\n           user=\"postgres\",\n           password=\"postgres\",\n           host=\"host.docker.internal\",\n           port=\"5432\"\n       )\n\n       cur = conn.cursor()\n\n       # Fun\u00e7\u00e3o para exportar dados para CSV\n       def export_to_csv(table_name, file_name):\n           with open(file_name, 'w') as f:\n               cur.copy_expert(f'COPY {table_name} TO STDOUT WITH CSV HEADER', f)\n\n       # Exportando as tabelas para arquivos CSV\n       export_to_csv('associacaos', './dags/csv/associacaos.csv')\n       export_to_csv('instituicaos', './dags/csv/instituicaos.csv')\n       export_to_csv('cursos', './dags/csv/cursos.csv')\n       export_to_csv('usuarios', './dags/csv/usuarios.csv')\n       export_to_csv('parametros', './dags/csv/parametros.csv')\n       export_to_csv('pagamentos', './dags/csv/pagamentos.csv')\n\n       # Fechando o cursor e a conex\u00e3o\n       cur.close()\n       conn.close()\n</code></pre>  Por ser a Landing-Zone, esses dados ser\u00e3o persistidos em seu formato bruto, para serem manipulados nas camadas seguintes.  <pre><code># Bucket | Nome do Arquivo | Caminho do Arquivo\n       client.fput_object('landing-zone', 'associacaos.csv', './dags/csv/associacaos.csv')\n       client.fput_object('landing-zone', 'instituicaos.csv', './dags/csv/instituicaos.csv')\n       client.fput_object('landing-zone', 'cursos.csv', './dags/csv/cursos.csv')\n       client.fput_object('landing-zone', 'usuarios.csv', './dags/csv/usuarios.csv')\n       client.fput_object('landing-zone', 'parametros.csv', './dags/csv/parametros.csv')\n       client.fput_object('landing-zone', 'pagamentos.csv', './dags/csv/pagamentos.csv')\n</code></pre></p>"},{"location":"executando/#bronzepy","title":"bronze.py","text":"<p>A camada Bronze utiliza Spark para ler e manipular os dados da landing-zone.  <pre><code># Lendo o arquivo do bucket landing-zone\n   df_associacaos = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/associacaos.csv\") \n   df_cursos = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/cursos.csv\") \n   df_instituicaos = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/instituicaos.csv\") \n   df_pagamentos = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/pagamentos.csv\") \n   df_parametros = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/parametros.csv\") \n   df_usuarios = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(f\"s3a://landing-zone/usuarios.csv\") \n</code></pre>  Para cada arquivo CSV, ser\u00e3o adicionadas novas colunas de metadados, marcando o momento do processamento,   e o nome do arquivo de origem.  <pre><code># Adicionando metadados de data e hora de processamento e nome do arquivo de origem\n   df_associacaos = df_associacaos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"associacaos.csv\"))\n   df_cursos = df_cursos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"cursos.csv\"))\n   df_instituicaos = df_instituicaos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"instituicaos.csv\"))\n   df_pagamentos = df_pagamentos.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"pagamentos.csv\"))\n   df_parametros = df_parametros.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"parametros.csv\"))\n   df_usuarios = df_usuarios.withColumn(\"data_hora_bronze\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"usuarios.csv\"))\n</code></pre>  Ap\u00f3s esse processo, os dados s\u00e3o persistidos na camada Bronze, j\u00e1 em formato Delta.  <pre><code># Salvando arquivo em formato delta no bucket bronze\n   df_associacaos.write.format(\"delta\").save(f\"s3a://bronze/associacaos\")\n   df_cursos.write.format(\"delta\").save(f\"s3a://bronze/cursos\")\n   df_instituicaos.write.format(\"delta\").save(f\"s3a://bronze/instituicaos\")\n   df_pagamentos.write.format(\"delta\").save(f\"s3a://bronze/pagamentos\")\n   df_parametros.write.format(\"delta\").save(f\"s3a://bronze/parametros\")\n   df_usuarios.write.format(\"delta\").save(f\"s3a://bronze/usuarios\")\n</code></pre></p>"},{"location":"executando/#silverpy","title":"silver.py","text":"<p>A camada Silver utiliza Spark para ler e manipular os dados da camada Bronze.  <pre><code>spark = SparkSession.builder.config(conf=conf).getOrCreate()\n\n# Lendo os arquivos da camada bronze\n   df_associacaos = spark.read.format(\"delta\").load(f\"s3a://bronze/associacaos/\")\n   df_cursos = spark.read.format(\"delta\").load(f\"s3a://bronze/cursos/\")\n   df_instituicaos = spark.read.format(\"delta\").load(f\"s3a://bronze/instituicaos/\")\n   df_pagamentos = spark.read.format(\"delta\").load(f\"s3a://bronze/pagamentos/\")\n   df_parametros = spark.read.format(\"delta\").load(f\"s3a://bronze/parametros/\")\n   df_usuarios = spark.read.format(\"delta\").load(f\"s3a://bronze/usuarios/\")\n</code></pre>  Cria\u00e7\u00e3o dos metadados (colunas) referentes a data de processamento dos dados, e nome do arquivo de origem.  <pre><code># Adicionando metadados de data e hora de processamento e nome do arquivo de origem\n   df_associacaos = df_associacaos.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"associacaos\"))\n   df_cursos = df_cursos.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"cursos\"))\n   df_instituicaos = df_instituicaos.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"instituicaos\"))\n   df_pagamentos = df_pagamentos.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"pagamentos\"))\n   df_parametros = df_parametros.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"parametros\"))\n   df_usuarios = df_usuarios.withColumn(\"data_hora_silver\", current_timestamp()).withColumn(\"nome_arquivo\", lit(\"usuarios\"))\n</code></pre>  Ap\u00f3s a cria\u00e7\u00e3o dos metadados, \u00e9 feita padroniza\u00e7\u00e3o dos nomes das tabelas, e exclus\u00e3o de tabelas desnecess\u00e1rias.  <pre><code>df_cursos = (\n   df_cursos\n   .withColumnRenamed(\"id\"                     , \"ID\")\n   .withColumnRenamed(\"nome\"                   , \"NOME\")\n   .withColumnRenamed(\"situacao\"               , \"SITUACAO\")\n   .withColumnRenamed(\"instituicao_id\"         , \"INSTITUICAO_ID\")\n   .drop(\"created_at\")\n   .drop(\"updated_at\")\n   .drop(\"data_hora_bronze\")\n   .withColumnRenamed(\"nome_arquivo\"           , \"NOME_ARQUIVO\")\n   .withColumnRenamed(\"data_hora_silver\"       , \"DATA_HORA_SILVER\")\n   )\n</code></pre>  As novas modifica\u00e7\u00f5es ser\u00e3o persistidas na camada Silver, j\u00e1 no formato Delta.  <pre><code># Salvando os arquivos na camada silver\n   df_associacaos.write.format(\"delta\").save(f\"s3a://silver/associacaos/\")\n   df_cursos.write.format(\"delta\").save(f\"s3a://silver/cursos/\")\n   df_instituicaos.write.format(\"delta\").save(f\"s3a://silver/instituicaos/\")\n   df_pagamentos.write.format(\"delta\").save(f\"s3a://silver/pagamentos/\")\n   df_parametros.write.format(\"delta\").save(f\"s3a://silver/parametros/\")\n   df_usuarios.write.format(\"delta\").save(f\"s3a://silver/usuarios/\")\n</code></pre></p>"},{"location":"executando/#goldpy","title":"gold.py","text":"<p>A camada Gold utiliza Spark para ler os dados da camada Silver.  <pre><code>spark = SparkSession.builder.config(conf=conf).getOrCreate()\n\n   # Lendo os arquivos da camada silver\n   df_associacaos = spark.read.format(\"delta\").load(f\"s3a://silver/associacaos/\")\n   df_cursos = spark.read.format(\"delta\").load(f\"s3a://silver/cursos/\")\n   df_instituicaos = spark.read.format(\"delta\").load(f\"s3a://silver/instituicaos/\")\n   df_pagamentos = spark.read.format(\"delta\").load(f\"s3a://silver/pagamentos/\")\n   df_parametros = spark.read.format(\"delta\").load(f\"s3a://silver/parametros/\")\n   df_usuarios = spark.read.format(\"delta\").load(f\"s3a://silver/usuarios/\") \n</code></pre>  Ser\u00e3o feitas modifica\u00e7\u00f5es em colunas espec\u00edficas dos dataframes, para que n\u00e3o haja ambiguidade no resultado.  <pre><code># Selecionando as colunas necess\u00e1rias e renomeando as colunas para evitar ambiguidades\n   df_associacaos_final = df_associacaos_clone.select(\"ID\", \"NOME\", \"SITUACAO\") \\\n       .withColumnRenamed(\"ID\", \"codigo_associacao\") \\\n       .withColumnRenamed(\"NOME\", \"nome_associacao\") \\\n       .withColumnRenamed(\"SITUACAO\", \"situacao_associacao\")\n\n   df_pagamentos_final = df_pagamentos_clone.select(\"ID\", \"DATA_VENCIMENTO\", \"VALOR\", \"MULTA\", \"SITUACAO\", \"USUARIO_ID\") \\\n       .withColumnRenamed(\"ID\", \"codigo_pagamento\") \\\n       .withColumnRenamed(\"DATA_VENCIMENTO\", \"data_vencimento\") \\\n       .withColumnRenamed(\"VALOR\", \"valor\") \\\n       .withColumnRenamed(\"MULTA\", \"multa\") \\\n       .withColumnRenamed(\"SITUACAO\", \"situacao\") \\\n       .withColumnRenamed(\"USUARIO_ID\", \"codigo_usuario\")\n</code></pre>  Ap\u00f3s manipular as colunas, os dataframes ser\u00e3o agregados em um \u00fanico dataframe.  <pre><code># Realizando a jun\u00e7\u00e3o dos dataframes\n   df_merged = df_pagamentos_final \\\n       .join(df_usuarios_final, df_pagamentos_final[\"codigo_usuario\"] == df_usuarios_final[\"codigo_usuario\"], \"inner\") \\\n       .join(df_associacaos_final, df_usuarios_final[\"codigo_associacao\"] == df_associacaos_final[\"codigo_associacao\"], \"inner\") \\\n       .select(df_pagamentos_final[\"codigo_pagamento\"],\n               df_pagamentos_final[\"data_vencimento\"],\n               df_pagamentos_final[\"valor\"],\n               df_pagamentos_final[\"multa\"],\n               df_pagamentos_final[\"situacao\"],\n               df_usuarios_final[\"codigo_usuario\"],\n               df_usuarios_final[\"nome_usuario\"],\n               df_usuarios_final[\"situacao_usuario\"],\n               df_usuarios_final[\"dias_uso_transporte\"],\n               df_associacaos_final[\"codigo_associacao\"],\n               df_associacaos_final[\"nome_associacao\"],\n               df_associacaos_final[\"situacao_associacao\"])\n</code></pre>  O resultado da jun\u00e7\u00e3o dos dataframes ser\u00e1 persistido na camada Gold  <pre><code>df_merged.write.format(\"delta\").save(f\"s3a://gold/modelo_eng_dados/\")\n</code></pre></p>"},{"location":"executando/#_1","title":"Executando o Projeto","text":""},{"location":"executando/#_2","title":"Executando o Projeto","text":"<p>Nessa parte descrita acima, acredito que pode ser colocado junto os comandos disponiveis nos arquivos, buckets.py, landzone.py, bronze.py,  silver.py e gold.py, os quais refltem todo o processo descrito acima como exemplifica\u00e7\u00e3o que est\u00e3o no caminho astro/dags</p>"},{"location":"geracaoDados/","title":"Gera\u00e7\u00e3o de Dados","text":"<p>Foi criado um banco de dados com o modelo incial do buson, criado um backend em java para fazer a conexcao com o banco apos isso utilizando as entidades em java e utilizando a dependencia do faker para criar dados ficticios e popular as tabelas. Apos isso foi pego scripts sql que foram gerados e inseriddos em um arquivo sql com a cricao das tabelas e insercao dos dados.  Na criacao dos ambientes do Airflow e minio o banco iniciado foi o postgres junto com o script de inicializacao para ja ter todos os dados inseridos, foi usado apenas o volume inicial para se obter o script.</p>"},{"location":"inicializando/","title":"Inicializando","text":""},{"location":"inicializando/#como-subir-o-ambiente-com-airflow-spark-e-minio","title":"COMO SUBIR O AMBIENTE COM AIRFLOW, SPARK E MINIO","text":"<ul> <li>Ferramentas necess\u00e1rias para a cria\u00e7\u00e3o do ambiente:</li> </ul>"},{"location":"inicializando/#docker","title":"Docker","text":"<p>Instale o Docker -&gt; CLIQUE AQUI</p>"},{"location":"inicializando/#git","title":"Git","text":"<p>Instale o Git -&gt; CLIQUE AQUI</p>"},{"location":"inicializando/#astro-cli","title":"Astro CLI","text":"<ul> <li>Abra o Windows PowerShell em modo Administrador e execute o seguinte comando:  <pre><code>   winget install -e --id Astronomer.Astro\n</code></pre></li> </ul> <p>Documenta\u00e7\u00e3o Astro CLI -&gt; CLIQUE AQUI</p>"},{"location":"inicializando/#clonando-o-projeto","title":"Clonando o Projeto","text":"<ul> <li>Crie uma pasta no seu Desktop, e navegue at\u00e9 essa pasta atrav\u00e9s do CMD.</li> <li>Ap\u00f3s navegar at\u00e9 a pasta, execute o seguinte comando no Terminal para clonar o reposit\u00f3rio:  <pre><code>git clone https://github.com/thiagoDimon/buson-bd-eng-dados.git\n</code></pre></li> </ul>"},{"location":"inicializando/#iniciando-as-imagens-do-container","title":"Iniciando as Imagens do Container","text":"<ul> <li>Abra o Docker Desktop, e certifique-se de estar logado em uma conta.</li> <li>Pelo CMD, navegue at\u00e9 a pasta astro, e execute o seguinte comando no Terminal:  <pre><code>astro dev start\n</code></pre></li> <li>Agora, basta aguardar (pode demorar um pouco).</li> </ul>"},{"location":"instalacao/","title":"Guia de Instala\u00e7\u00e3o","text":""},{"location":"preRequisitos/","title":"Pr\u00e9-Requisitos para o Projeto","text":"<p>Para garantir o funcionamento adequado do projeto, certifique-se de que todos os seguintes pr\u00e9-requisitos est\u00e3o atendidos e instalados:</p>"},{"location":"preRequisitos/#ferramentas-necessarias","title":"Ferramentas Necess\u00e1rias","text":"<ol> <li> <p>SPARK 3.3.1</p> <ul> <li>Utilizado para o processamento distribu\u00eddo de dados.</li> </ul> </li> <li> <p>DELTA 2.3.0</p> <ul> <li>Formato de armazenamento que permite a leitura e escrita de dados de forma eficiente com o Apache Spark.</li> </ul> </li> <li> <p>HADOOP 3.2.2</p> <ul> <li>Framework necess\u00e1rio para o funcionamento do Spark e Delta.</li> </ul> </li> <li> <p>MINIO OBJECT STORAGE</p> <ul> <li>Armazenamento de objetos onde os arquivos ser\u00e3o salvos.</li> </ul> </li> <li> <p>ASTRO CLI</p> <ul> <li>Interface de linha de comando para orquestra\u00e7\u00e3o de dados.</li> </ul> </li> <li> <p>AIRFLOW</p> <ul> <li>Ferramenta de orquestra\u00e7\u00e3o de workflows para agendar e monitorar os pipelines de dados.</li> </ul> </li> <li> <p>DOCKER</p> <ul> <li>Ferramenta para criar e gerenciar containers, utilizada para subir o Airflow, MinIO e o cluster do Spark.</li> </ul> </li> </ol>"},{"location":"preRequisitos/#ferramentas-adicionais-opcional","title":"Ferramentas Adicionais (Opcional)","text":"<p>As seguintes ferramentas foram utilizadas durante o desenvolvimento do projeto, mas n\u00e3o s\u00e3o necess\u00e1rias para a execu\u00e7\u00e3o:</p> <ul> <li> <p>GIT</p> <ul> <li>Sistema de controle de vers\u00e3o utilizado para clonar o reposit\u00f3rio do projeto.</li> </ul> </li> <li> <p>Java</p> <ul> <li>Utilizado para algumas depend\u00eancias do Spark.</li> </ul> </li> <li> <p>Faker</p> <ul> <li>Biblioteca para gera\u00e7\u00e3o de dados falsos para testes e desenvolvimento.</li> </ul> </li> </ul> <p>Certifique-se de que todas as ferramentas necess\u00e1rias est\u00e3o instaladas e configuradas corretamente antes de iniciar o projeto.</p>"}]}