{"config":{"lang":["pt"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Projeto Engenharia de Dados","text":"<p>Trabalho desenvolvido para a disciplina de Engenharia de dados do Curso de Engenharia da Software da UNISATC. A proposta do projeto e desenvolver uma pipeline de engenharia de dados...</p>"},{"location":"#principais-comandos-do-mkdocs","title":"Principais comandos do MkDocs","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Cria um novo projeto.</li> <li><code>mkdocs serve</code> - Inicia o preview das paginas *.md da pastas /docs.</li> <li><code>mkdocs build</code> - Cria a estrutura de paginas web no padrao hmtl, css, js.</li> <li><code>mkdocs gh-deploy</code> - Publica as paginas criadas pelo 'mkdocs build' na estrutura do github pages.</li> </ul>"},{"location":"#documentacao-para-referencia-e-estudo","title":"Documentacao para referencia e estudo","text":"<p>https://tutorial-mkdocs.systemhealthlab.com/flavoured_markdown.html https://squidfunk.github.io/mkdocs-material/reference/</p>"},{"location":"configuracoesAirflow/","title":"Configura\u00e7\u00f5es Airflow e Minio","text":"<ul> <li>Acessar a seguinte url para realizar a execu\u00e7\u00e3o das DAGS do Airflow: http://localhost:8080/</li> <li>Usu\u00e1rio e senha de acesso ao airflow: admin e admin</li> <li>Configurar a conex\u00e3o com o cluster do spark na guia admin e posteriormente em conections (imagem da configura\u00e7\u00e3o enviada pelo Zanoni)</li> </ul>"},{"location":"configuracoesAirflow/#variaveis-de-ambiente-minio","title":"V\u00e1riaveis de Ambiente Minio","text":"<ul> <li>Acessar o minio na porta 9001 usuario: minioadmin senha: minioadmin</li> <li>Ir em AccessKeys e criar uma nova, copiar o accessKey e o accessSecretKey e colocar no dotenv do projeto. Apos isso reiniciar o ambiente com o comando  <pre><code>astro dev restart\n</code></pre></li> </ul>"},{"location":"configuracoesDocker/","title":"Configura\u00e7\u00f5es Docker","text":""},{"location":"configuracoesDocker/#dockerfile","title":"Dockerfile","text":"<pre><code>- Neste arquivo foi necess\u00e1rio realizar a instala\u00e7\u00e3o do manipulador do minio\n- Instala\u00e7\u00e3o do providers do apache spark para realizar a conex\u00e3o com o airflow\n- Instala\u00e7\u00e3o da biblioteca pandas para manipula\u00e7\u00e3o dos dataframes\n- Instala\u00e7\u00e3o do Java e configura\u00e7\u00e3o das vari\u00e1veis de ambiente para funcionamento do SPARK\n</code></pre>"},{"location":"configuracoesDocker/#docker-compose-override","title":"Docker Compose Override","text":"<pre><code>- Realizado a inser\u00e7\u00e3o da imagem do minio e configura\u00e7\u00e3o da subnet para que o airflow consiga lhe enxergar\n- Realizado a inser\u00e7\u00e3o da imagem do cluster do spark e configura\u00e7\u00e3o da subnet para que o airflow consiga lhe enxergar\n- Na imagem do postgres adicionado os volumes do banco de dados buson_bd para realizar a cria\u00e7\u00e3o das tabelas e inser\u00e7\u00e3o autom\u00e1tica dos dados de exemplo\n</code></pre>"},{"location":"dashboard/","title":"Dashboard","text":""},{"location":"dashboard/#colocar-informacoes-sobre-o-power-bi","title":"COLOCAR INFORMACOES SOBRE O POWER BI","text":""},{"location":"executando/","title":"Executando o projeto","text":"<ul> <li>Executar as seguintes DAGS na ordem aqui fornecida: </li> </ul>"},{"location":"executando/#minio_buckets","title":"minio_buckets:","text":"<p>realiza a cria\u00e7\u00e3o dos buckets landing-zone, bronze, silver e gold no nosso object storage  para persistir os nossos dados que ser\u00e3o utilizados para manipula\u00e7\u00e3o</p>"},{"location":"executando/#landing-zone","title":"landing-zone:","text":"<p>extrai as informa\u00e7\u00f5es do banco de dados criado dentro do nosso container em formato csv para manipula\u00e7\u00e3o com o spark  e persiste nesta mesma camada este arquivos</p>"},{"location":"executando/#bronze","title":"bronze","text":"<p>realizar a leitura dos arquivos da camada landing-zone e manipula\u00e7\u00e3o de dados inserindo metadados como hora de processamento e arquivo de origem</p>"},{"location":"executando/#silver","title":"silver","text":"<p>realiza a leitura dos arquivos persistidos em formato delta na bronze e realiza manipula\u00e7\u00f5es nos arquivos  padronizando nomes de tabelas e deletando dados desnecess\u00e1rios</p>"},{"location":"executando/#gold","title":"gold","text":"<p>realiza a leitura dos arquivos persistidos em formato delta na silver e realiza manipula\u00e7\u00f5es dos dados a fim de  disponibilizar os dados tratados no modelo One Big Table, encaminhando e persistindo novamente no banco de dados para consumo do Power BI</p>"},{"location":"executando/#_1","title":"Executando o Projeto","text":""},{"location":"executando/#_2","title":"Executando o Projeto","text":"<p>Nessa parte descrita acima, acredito que pode ser colocado junto os comandos disponiveis nos arquivos, buckets.py, landzone.py, bronze.py,  silver.py e gold.py, os quais refltem todo o processo descrito acima como exemplifica\u00e7\u00e3o que est\u00e3o no caminho astro/dags</p>"},{"location":"geracaoDados/","title":"Gera\u00e7\u00e3o de Dados","text":"<p>Foi criado um banco de dados com o modelo incial do buson, criado um backend em java para fazer a conexcao com o banco apos isso utilizando as entidades em java e utilizando a dependencia do faker para criar dados ficticios e popular as tabelas. Apos isso foi pego scripts sql que foram gerados e inseriddos em um arquivo sql com a cricao das tabelas e insercao dos dados.  Na criacao dos ambientes do Airflow e minio o banco iniciado foi o postgres junto com o script de inicializacao para ja ter todos os dados inseridos, foi usado apenas o volume inicial para se obter o script.</p>"},{"location":"inicializando/","title":"Inicializando","text":""},{"location":"inicializando/#como-subir-o-ambiente-com-airflow-spark-e-minio","title":"COMO SUBIR O AMBIENTE COM AIRFLOW, SPARK E MINIO","text":"<ul> <li>Necess\u00e1rio nessa etapa possui instalado em seu desktop Astro CLI, Docker e Git</li> <li>Realizar o clone do projeto https://github.com/thiagoDimon/buson-bd-eng-dados.git com o comando git clone</li> <li>Abrir o Docker Desktop</li> <li>Abrir um terminal no clone do projeto e acessar a pasta astro e rodar o comando astro dev start para iniciar as imagens do container</li> <li>Aguardar o up do projeto</li> </ul>"},{"location":"instalacao/","title":"Guia de Instala\u00e7\u00e3o","text":""},{"location":"preRequisitos/","title":"Pr\u00e9-Requisitos para o Projeto","text":"<ul> <li>SPARK 3.3.1</li> <li>DELTA 2.3.0</li> <li>HADOOP 3.2.2</li> <li>MINIO OBJECT STORAGE (LUGAR PARA SALVAR NOS ARQUIVOS)</li> <li>ASTRO CLI (INTERFACE DE LINHA DE COMANDOS PARA ORQUESTRA\u00c7\u00c3O DE DADOS)</li> <li>AIRFLOW (FERRAMENTA DE ORQUESTRA\u00c7\u00c3O)</li> <li>DOCKER (FERRAMENTA DAS IMAGENS UTILIZADOS PARA SUBIR O AIRFLOW, MINIO E CLUSTER DO SPARK)</li> <li>GIT (PARA REALIZAR O CLONE DO PROJETO)</li> </ul> <p>Foi utilizado, por\u00e9m nao tem necessidade  - Java - Faker</p>"}]}